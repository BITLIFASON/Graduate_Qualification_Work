# Bachelor's degree graduate qualification work

Тема: Исследование нейросетевых моделей и алгоритмов машинного обучения для обработки текстовых документов.

В данном описании представлена только часть работы из-за её объёмности.

# **Оглавление**

0. [Введение](#intro)

1. [Исследование методов машинного обучения и нейросетевых архитектур для классификации текстовых документов](#ML_DL)

   1.1. [Метод опорных векторов](#SVM)
  
   1.2. [Случайный лес](#RF)
  
   1.3. [Градиентный бустинг](#GB)
  
   1.4. [Полносвязные нейросети прямого распространения](#FFNN)
  
   1.5. [Рекуррентные нейросети](#RNN)
  
   1.6. [Свёрточные нейросети](#CNN)
  
   1.7. [Архитектура Encoder-Decoder](#Enc-Dec)
  
   1.8. [Комбинированные нейросети](#Comb)

2. [Сравнительный анализ моделей представления текстовых документов](#emb)

   2.1. [Мешок слов](#BoW)

   2.2. [Латентный семантический анализ](#LSA)

   2.3. [Word2Vec и родственные ему модели](#Word2Vec)
  
   2.4. [ELMo](#ELMo)
  
   2.5. [BERT](#BERT)

4. [Описание корпусов текстовых документов для проведения исследований](#corp)
5. [Обзор библиотек для обработки и анализа текстовых документов](#lib)
6. [Применение моделей представления текстовых документов и классификаторов к выбранным корпусам текстовых документов](#res)
7. [Заключение](#conc)
8. [Список использованных источников](#lit)


# **Введение** <a name="intro"></a>

В настоящее время объёмы текстовой информации растут чрезвычайно быстро. При этом анализ такого большого количества текстовой информации вручную невозможен, вследствие чего в настоящее время ускоренно развиваются методы обработки естественного языка.

Большинство таких методов относится либо к машинному обучению, либо к глубокому обучению.

Научно-исследовательская работа посвящена исследованию методов машинного обучения и нейросетевых моделей для решения одной из самых распространённых задач обработки текстовых документов — задаче классификации текстовых документов, которая имеет множество приложений: от классификации тональности отзывов к фильмам до классификации запросов на обслуживание центром поддержки.

Исследование данной задачи представляет собой несколько этапов:
- изучение методов машинного обучения и нейросетевых архитектур для классификации текстовых документов;
- анализ проблемы, связанной с компьютерной обработкой естественных языков, и подходов к её решению;
- выбор корпусов текстовых документов;
- программная реализация моделей;
- оценка результатов.

Итогом работы является сравнение обученных моделей на выбранных корпусах текстовых документов.


# **Исследование методов машинного обучения и нейросетевых архитектур для классификации текстовых документов** <a name="ML_DL"></a>

Классификация текстовых документов является классической задачей обработки естественного языка (NLP), которая заключается в присвоении меток текстовым единицам, таким как предложения, абзацы или документы. 

Она имеет широкий спектр приложений, включая ответы на вопросы (Question Answering), анализ настроений (Sentiment Analysis), тематический анализ (Topic Analysis), определение логической связи между текстами (Natural Language Inference) и так далее.

Отличительной особенностью задач, связанных с NLP, является проблема представления текстовых данных в виде набора чисел (обычно вектор, набор векторов, либо матрица), который можно использовать в математических моделях, поэтому исследователями были разработаны множество моделей представления текстовых данных.

Долгое время в основных методах классификации в задачах NLP доминировали методы на основе правил (Rule-based methods) и подходы машинного обучения (Machine learning based methods). Методы на основе правил классифицируют текст, используя набор предопределенных правил, основанных на глубоких знаниях предметной области. 

Подходы, основанные на машинном обучении, работают на основе наблюдений за данными, используя предварительно размеченные примеры в качестве обучающих данных, алгоритм машинного обучения изучает неотъемлемые ассоциации между текстами и их метками.

С ростом количества текстовых данных обнаружилось, что эти методы не могут в полной мере использовать большие объемы обучающих данных, вследствие чего стали ускоренно развиваться нейросетевые модели.

В настоящее время существует множество архитектур нейросетевых моделей, однако в большинстве случаев для задачи классификации текста применяются следующие: полносвязные нейросети прямого распространения, реккурентные нейросети, свёрточные нейросети, архитектура Encoder-Decoder, комбинированные нейросети.

Обучение нейросети сводится к задаче минимизации функции потерь, в качестве которой чаще всего выступает перекрёстная энтропия, представленная выражением. Её выбор во многом обуславливается проблемой исчезающего градиента, который характерен для глубоких нейросетей, однако в “неглубоких” нейросетях, например, в многослойном персептроне можно использовать и другие функции потерь.

Самым распространённым методом обучения нейронных сетей является метод обратного распространения ошибки. Суть его заключается в следующем: если на выходе сети имеется отклонение от правильного ответа, то эта ошибка распространяется обратно по сети и весовые коэффициенты меняются так, чтобы уменьшить ошибку.


## **Метод опорных векторов** <a name="SVM"></a>

Метод опорных векторов (Support Vector Machine, SVM) был разработан в 1964 году Вапником и Червоненкисом, как линейный метод классификации, но в 1992 году Бозер адаптировал его и к нелинейной классификации. Изначально SVM был разработан для бинарной классификации, однако в настоящее время существуют его реализации и для многоклассовой классификации.


## **Случайный лес** <a name="RF"></a>

Алгоритм дерево решений (Decision Tree) был разработан в 1963 году Морганом и Сонквистом. Он заключается в разбиении пространства признаков на подпространства с помощью иерархической структуры в виде дерева, ветвление на узлы (листы) в котором происходит на основе критерия информативности, в качестве которого выступают: ошибка классификации, энтропийный критерий, критерий Джини.

Предсказание класса объекта происходит на основе применения решающих правил дерева и выбора наиболее распространённого класса в последнем неделимом (терминальном) узле.

Хо в 1995 году разработал алгоритм лес случайных решений, основанный на идее построения нескольких деревьев решений, каждый в своём случайно выбранном подпространстве пространства признаков, которые обобщают свою классификацию взаимодополняющими способами. В 1996 году Брейман представил идею бутстрэп-агрегирования (Bootstrap Aggregating, Bagging) для деревьев решений, заключающуюся в формировании случайным образом нескольких подмножеств из множества исходных данных, которые используются для обучения отдельных независимых деревьев решений, после чего проводится голосование на основе их предсказаний. В 1999 году он объединил эти идеи и представил алгоритм случайный лес (Random Forest).


## **Градиентный бустинг** <a name="GB"></a>

Бустинг представляет собой семейство ансамблевых алгоритмов, которое основано на гипотезе бустинга представленной Кернсом и Валиантом: "Подразумевает ли эффективный алгоритм обучения, который выдает гипотезу, производительность которой лишь немного лучше, чем случайное угадывание, существование эффективного алгоритма, который выдает гипотезу произвольной точности.". 

Шапир доказал данную гипотезу и представил метод улучшения производительности слабого алгоритма обучения (a recursive majority gate formulation). В качестве слабых алгоритмов обычно используются деревья решений. Кроме оригинального алгоритма на данный момент существует множество более современных алгоритмов бустинга, в основе которых лежит концепция комбинации слабых алгоритмов в один сильный алгоритм, однако самым распространённым из них является AdaBoost.

Идея градиентного бустинга была введена Брейманом и заключается в интерпретации бустинга, как алгоритма оптимизации функции потерь. Впоследствии Фридманом был разработан алгоритм градиентного бустинга (Gradient Boosting Machine, GBM), основанный на оптимизации функции потерь в функциональном пространстве (пространстве алгоритмов).


## **Полносвязные нейросети прямого распространения** <a name="FFNN"></a>

Сети данной архитектуры представляют собой набор слоёв, где каждый слой представляет собой совокупность нейронов, при чём каждый нейрон следующего слоя связан со всеми нейронами предыдущего. Самой распространённой сетью данной архитектуры является многослойный персептрон (MLP). Такие сети не имеют обратных связей, то есть в них информация идёт только от входа сети к её выходу.

Входной слой предназначен для подачи в сеть некоторого “числового” представления текста фиксированной длины. Выходной слой используется для получения результатов классификации, количество нейронов в нём равно количеству классов для многоклассовой классификации, или только одному для бинарной классификации. Количество скрытых слоёв и нейронов в них подбирается экспериментальным путём.

Под нейроном понимается вычислительная единица, имеющая скалярные входы и выходы. Нейрон умножает каждый входной сигнал на соответствующий ему весовой коэффициент, суммирует их и к результату применяет нелинейную функцию активации, после чего передает его на свой выход. 


## **Рекуррентные нейросети** <a name="RNN"></a>

С текстовыми данными можно работать как с последовательностями, такими как последовательности букв, слов и предложений, и порядок в этих последовательностях может иметь некий смысл, например, разные порядки слов могут обозначать разные значения.

Рекуррентные нейронные сети (RNNs) обрабатывают входные данные в виде последовательности и изучают упорядоченные конструкции из текста, поэтому они хорошо подходят для анализа более длинных языковых единиц, таких как фразы, предложения, абзацы или документы.

Идея рекуррентности заключается в том, что при обработке входной последовательности сохраняется информация о всей предыдущей части последовательности в скрытом состоянии ячейки памяти (hidden state).

Входные данные представляют собой набор векторов, где каждый вектор соответствует какой-либо единице текста, например, символ, набор символов, слово, предложение. Каждый такой вектор подаётся в отдельную ячейку памяти, что позволяет использовать последовательности произвольной длины, и так как рекуррентные сети позволяют представлять входные данные в векторе фиксированного размера, классификация текстовых документов сводится к подаче скрытого состояния последней ячейки памяти в сеть прямого распространения.


## **Свёрточные нейросети** <a name="CNN"></a>


Свёрточная нейронная сеть (CNN) – однонаправленная многослойная нейросеть с применением операции свертки (convolution), которая заключается в поэлементном умножении фрагмента входных данных на ядро свёртки (kernel), после чего результаты суммируются и записывается в соответствующую позицию выходных данных, которые называются картой признаков (feature map). Количество ядер и слоёв в сети может быть произвольным.

Изначально CNN разрабатывалась для обработки изображений и видео (Computer Vision), как сеть, которая учится абстрагировать входные данные и обнаруживать разноуровневые шаблоны. Данные сети имеют большую вычислительную сложность, поэтому для частичного решения этой проблемы используют субдискретизацию (pooling), который уменьшает размер данных между слоями. Существует множество вариантов субдискретизации, однако самыми распространёнными являются max-pooling и average-pooling, которые заключаются в извлечении максимального элемента из последовательности и усреднении всей последовательности соответственно.

Идея использования данных сетей для анализа текстовых данных заключается в том, чтобы анализировать сочетания слов, так как они могут иметь разную информативность, независимо от их расположения в предложении. Изображения и представления текстовых документов в качестве входных данных имеют существенные различия, поэтому для решения задач NLP алгоритм работы свёрточных сетей был модифицирован.


## **Архитектура Encoder-Decoder** <a name="Enc-Dec"></a>

Сети архитектуры Encoder-Decoder состоят из двух частей, соответственно энкодер (encoder) и декодировщик или декодер (decoder). Задача энкодера преобразовать входные данные в некоторое скрытое представление, которое является более сжатым и информативным [28]. Декодер предназначен для осуществления преобразования выхода энкодера, в представление, требуемое для решения задачи.

Представителями архитектуры Encoder-Decoder являются автоэнкодеры. Данные сети обучаются воспроизводить на выходе входные данные.

Автоэнкодеры используются в качестве метода уменьшения размерности, так как скрытый слой между входным и выходным слоями имеет меньшую размерность, чем входной слой.
В качестве энкодера и декодера могут выступать модели всех рассмотренных ранее нейросетевых архитектур. Эти модели широко используются в Seq2Seq задачах, таких как машинный перевод, когда из входной последовательности нужно получить требуемую выходную.

Для получения более информативного представления входных данных в задачах NLP был разработана концепция внимания (attention). Данная концепция создана для улавливания зависимостей между далеко стоящими словами и основана на том, как люди читают и понимают длинные тексты. Также она позволяет фокусироваться на частях текста, которые имеют более высокую важность для текущей задачи.

Механизм внимания оценивает, насколько сильно слово коррелирует или “обращает внимание” на другие слова, и берёт сумму их представлений, взвешенных вектором внимания, в качестве нового представления слова.

Одно из главных достоинств данного механизма заключается в том, что он может быть реализован в большинстве нейросетевых архитектур.
Развитием идеи внимания стала разработка новой нейросетевой архитектуры, которая называется трансформер (Transformer). Она представляет собой автоэнкодер, у которого энкодер и декодер представляет собой совокупность слоёв с несколькими подслоями и дополнительными соединениями.


## **Комбинированные нейросети** <a name="Comb"></a>

Рассмотренные раннее нейросетевые архитектуры можно объединять для создания моделей с более высокими показателями качества решения задачи.


# **Сравнительный анализ моделей представления текстовых документов** <a name="emb"></a>

Текстовые данные являются чрезвычайно ценным и богатым источником информации для использования во многих приложениях. Тем не менее, извлечение информации из текста является сложной задачей, решение которой может занимать много времени из-за неструктурированного характера текста.

Чтобы текст можно было использовать в качестве входных данных, его сначала необходимо преобразовать в подходящее представление [41], которое обычно представляет собой вектор его свойств, в котором может содержаться семантика, синтаксис и прочие свойства языка. Такие векторные представления принято называть эмбидингами (embeddings).
Перед преобразованием текста в “числовой” вид его необходимо обработать. Предварительная обработка происходит в несколько стадий: токенизация — разбиение текста на отдельные смысловые лексические единицы (чаще всего слова), лемматизация — приведение токенов к нормальной форме или стеминг — извлечение основ токенов, удаление стоп-слов.

Эмбидинг может быть получен для любой единицы текста (токена), такой как символы, набор символов, слова, фразы, предложения, абзацы и документы. Основное требование к эмбидингам состоит в том, чтобы они не зависели от решаемой задачи, то есть содержали только общие свойства единицы текста.
Получаются эмбидинги с помощью моделей представления. Существует много различных моделей представления, основанных на методах взвешивания, то есть на подсчёте частот токенов в тексте, и нейросетевых методах.


## **Мешок слов** <a name="BoW"></a>

Мешок слов (Bag of Words, BoW) это одно из самых простых представлений текстового документа, основанное на частоте слов (терминов). BoW рассматривает каждое слово, как one-hot вектор размерности словаря, то есть каждому слову отводится отдельная позиция, и на позиции соответствующего слова стоит единица, а на всех остальных нули. Под словарём понимается набор всех слов в корпусе документов.

Эмбидинг документа получается суммированием векторов слов, входящих в него. В результате получается вектор, в котором каждому слову из словаря соответствует количество вхождений этого слова в документ (term frequency).

К недостаткам BoW относятся: вектора не отражают семантику слов, нет информации о порядке слов, большая размерность векторов.


## **Латентный семантический анализ** <a name="LSA"></a>

Латентный семантический анализ (LSA) был разработан в 1989 и основан на статистических методах. Он позволяет выявлять скрытые связи между набором документов и словами, в них встречающимися.

В качестве входных данных LSA использует матрицу документ-термин, строки которой соответствуют документам, а столбцы терминам. Элементы этой матрицы являются весами, которые получаются в результате работы метода взвешивания. Как правило, в качестве метода взвешивания используется Term Frequency — Inverse Document Frequency (TF-IDF), 

Чем больше вес, тем важнее слово с индексом   в документе под номером  . 

Данный метод взвешивания частично решает проблему неинформативных слов, так как он учитывает частоту встречаемости термина во всех документах.
Наиболее распространенный вариант LSA основан на использовании сингулярного разложения матрицы (SVD). С помощью SVD-разложения матрицу можно разложить на несколько матриц, линейная комбинация которых является точным приближением к исходной матрице.

Основными недостатками данного метода являются низкая скорость вычислений при больших объёмах входных данных и необходимость производить все вычисления заново при небольших изменениях в корпусе текстов.


## **Word2Vec и родственные ему модели** <a name="Word2Vec"></a>

Подходы использующие методы взвешивания дают только синтаксические представления слов, однако важную роль имеет семантическое значение.

Одним из первых подходов для представления текстовых документов, основанный на нейронных сетях, является Word2Vec. Его отличие от более ранних подходов заключается в том, чтобы представлять каждое слово не как уникальное измерение, а в виде вектора в некотором общем непрерывном пространстве признаков.

Основная идея состоит в том, что слово зависит от контекста, в котором оно употребляется. К Word2Vec относятся модели Continuous Bag of Words (CBoW) и Skip-Gram. Они представляют собой двуслойную полносвязную нейросеть прямого распространения, которая обучается методом обратного распространения ошибки. В качестве входных данных в сеть подаются one-hot вектора слов.

Модель CBoW проецирует вектора контекстных слов в вектор центрального слова, а Skip-Gram наоборот вектор центрального слова в вектора контекстных слов.

Формально обучение этих моделей можно представить в виде двух задач. Максимизации условной вероятности контекстных слов на основе центрального для Skip-gram, и наоборот максимизации условной вероятности центрального слова на основе контекстных для CBoW.

Использование контекста в процессе обучения позволило учитывать не только синтаксические свойства слов, но и семантические.

Дальнейшим развитием подхода Word2Vec стали модели Global Vectors for Word Representation (GloVe) и FastText.

GloVe сочетает в себе разложение матрицы совместной встречаемости слов в корпусе документов и идею контекста Word2Vec с помощью взвешенной регрессионной модели наименьших квадратов. Идея этой модели заключается в изучении представлений слов, на сновании матрицы совместной встречаемости таким образом, чтобы она могла прогнозировать соотношение вероятностей совместной встречаемости слов.

Модели Word2Vec и GloVe, присваивая каждому слову отдельный вектор, игнорируют морфологию слов, однако она может содержать дополнительную информацию. Модель FastText, основанная на модели Skip-Gram, решает эту проблему с помощью разделения слов на набор n-грамм символов или по-другому подслов. N-грамма символов это последовательность из n символов, идущих подряд.

Основным недостатком данных моделей является то, что все вектора слов являются фиксированными, однако одно и то же слово может иметь разный смысл в различных контекстах.


## **ELMo** <a name="ELMo"></a>

Дальнейшим продолжением подхода Word2Vec является модель Context2Vec. Она использует Bi-LSTM и выдаёт контекстно-зависимое представление слова (Contextualized Word Representation). 

Главной особенностью данной модели представления текста является создание эмбидинга слова на основании его контекста не на стадии обучения, а в процессе использования модели. Таким образом, одно и то же слово может иметь разный вектор представления в разных контекстах.

Развитием такого представления текста является глубокое контекстно-зависимое представление слова (Deep Contextualized Word Representation), которое выдаёт модель Embeddings from Language Models (ELMo). Такое представление содержит не только общие характеристики употребления слова, такие как синтаксис и семантика, так и то, как это употребление варьируется в разных контекстах, то есть решает проблему омонимии.

ELMo представляет собой двунаправленную языковую модель (biLM). Задача языковых моделей (language models) заключается в оценивании вероятности токена или последовательности токенов на основании других токенов. Качество такой модели определяется тем, насколько много она содержит знаний о языке, что определяется задачами языкового моделирования.

Если обучить ELMo на задачах языкового моделирования, то получится предварительно обученная двунаправленная языковая модель.

К недостаткам ELMo относятся: учёт лёгких зависимостей, трудность обучения, проблема забывания информации.


## **BERT** <a name="BERT"></a>

Модель представления под названием Bidirectional Encoder Representation from Transformers (BERT) состоит из набора слоёв энкодера архитектуры трансформер. Отличительной особенностью данной модели является объединение двух идей: механизма внимания и предварительного обучения языковых моделей. Attention используется для представления левого и правого контекста слов.

Модель имеет две конфигурации BERT_BASE (L=12, H=768, A=12, Total Parameters=110M) и BERT_LARGE (L=24, H=1024, A=16, Total Parameters=340M), где L — количество слоёв, H — размер скрытых представлений, A — количество голов внимания.

В качестве предварительной обработки входных “сырых” текстовых данных модель использует WordPiece токенизацию, которая заключается в том, что изначально словарь состоит из всех отдельных символов в языке, а затем в него последовательно добавляются наиболее вероятные комбинации символов.

После токенизации каждому токену ставится в соответствие его эмбидинг, далее к ним прибавляются эмбидинги предложений (опционально) и позиционные эмбидинги. Словарь также содержит служебные токены (тэги), такие как [CLS] предназначенный решения задач классификации, [SEP] для разделения двух смысловых частей текста и решения ряда задач, [PAD] для представления токенов паддинга, [UNK] для неизвестных символов.

Обучение модели BERT происходит в два этапа: pre-training и fine-tuning. На этапе pre-training модель обучается на неразмеченных данных двум задачам языкового моделирования, таким как задача Клозе и предсказание следующего предложения (Next Sentence Prediction). Задача Клозе заключается в предсказании предварительно скрытой (маскированой) части текста, а Next Sentence Prediction в предсказании является ли следующее предложение следующим на самом деле. Этап fine-tuning заключается дообучении модели на конкретной задаче. 

Несмотря на высокое качество решения задач основным недостатком данной модели в её исходном варианте является ограничение на 512 токенов из-за чего её затруднительно применять к очень длинным текстам.


# **Описание корпусов текстовых документов для проведения исследований** <a name="corp"></a>

Для проведения исследований по оцениванию моделей представления и классификации были отобраны на основе сложности текстовых документов и их количества две коллекции текстовых документов в области науки.

Корпус Web of Science (WOS) содержит данные 46985 статей из базы данных Web of Science. Автоматический сбор данных производился с помощью Selenium с Chromedriver для веб-браузера Chrome. Для извлечения данных с сайта использовалась библиотека Beautiful Soup. Из собранного набора статей извлечены аннотация, предметная область и ключевые слова. Метки классов состоят из двух уровней. Первый уровень соответствует научным областям: Информатика (Computer Science), Электротехника (Electrical Engineering), Психология (Psychology), Механика (Mechanical Engineering), Строительство (Civil Engineering), Медицина (Medical Science), Биохимия (biochemistry). Второй уровень соответствует ключевым словам для каждой области.

Корпус Ohsumed, составленный Уильямом Хершем, содержит данные клинических статей из базы данных Medline, состоящей из названий и/или тезисов из 270 медицинских журналов за период с 1987 до 1991 года. Из собранного набора статей извлечены аннотация и термины, которые соответствуют тезаурусу заголовков медицинских дисциплин (Medical Subject Headings или MeSH). Изначально он создавался для решения задачи информационного поиска, но позже на его основе был создан датасет из 50216 статей, размеченные на 23 категории MeSH сердечно-сосудистых заболеваний.


# **Обзор библиотек для обработки и анализа текстовых документов** <a name="lib"></a>

Поскольку искусственный интеллект внедряется во всё большее число сфер жизни, существует потребность в разработке программного обеспечения для проведения исследований в этой области.

Для решения задач искусственного интеллекта существует множество библиотек для разных языков программирования, однако самым популярным языком программирования для таких задач является Python. Его достоинствами являются простой синтаксис и прозрачная семантика, также он легко интегрируется с другими языками программирования и инструментами, что сделало его предпочтительным для большинства исследователей в этой области. В результате чего сформировалась обширная коллекция мощных библиотек для решения задач многих областей искусственного интеллекта, в том числе и NLP.

Основным функционалом библиотек для решения задач NLP является упрощение предварительной обработки текста, чтобы исследователи могли сразу перейти к построению моделей и их обучению. Кроме того, библиотека должна иметь простой в освоении интерфейс взаимодействия с ней (API), а также способной эффективно реализовывать новейшие и лучшие алгоритмы и модели.

На данный момент накопилось огромное количество библиотек для решения задач NLP, относящихся к свободному программному обеспечению и доступных на операционных системах Windows, MacOS и Linux, поэтому приведены только наиболее популярные.

Scikit-learn является одной из самых распространённых библиотек для машинного обучения, построенная на основе SciPy. Она начала создаваться Дэвидом Курнапо в 2007 году, как проект Google Summer of Code, но наибольший вклад в её развитие внесли добровольцы, которыми она до сих пор и обслуживается. Она предоставляет различные инструменты для обучения модели, предварительной обработки данных, выбора модели, проверки модели и многих других утилит, которые могут применяться в том числе и для решения задач NLP. Она содержит отличную документацию по API и руководства по использованию. Так как Scikit-learn ориентирован на машинное обучение, то в ней отсутствуют нейросетевые модели, что может сделать её непригодной для решения задачи при наличии огромного количества текстовых документов.

PyTorch основан на библиотеке для языка программирования Lua под названием Torch и была представлена компанией Facebook в 2016 году. Они предназначены для построения новых нейросетевых моделей, но также содержат набор готовых моделей для решения задач из различных областей глубокого обучения. Модели — наборы арифметических операций над многомерными данными (тензорами), представленные в виде графов вычислений, для их обучения фреймворки используют автоматическое дифференцирование.

Библиотеки Transformers, Datasets и Tokenizers, разработанные компанией “Hugging Face” в 2018 году, предоставляют API-интерфейсы для скачивания и использования предварительно обученных моделей на основе PyTorch и TensorFlow, наборов данных и токенизаторов соответственно для различных областей глубокого обучения. Данные библиотеки взаимодействуют с платформой компании, которая поддерживается сообществом и другими компаниями, вследствие она содержит самые большое количество моделей и датасетов. К главному недостатку библиотек можно отнести трудности при поиске моделей и датасетов, так как у многих из них отсутствует документация.

Natural Language Toolkit (NLTK) является ведущей библиотекой для обработки естественного языка и используется как основной инструмент в NLP. Она создавалась как академический проект в Пенсильванском университете Стивеном Бердом и Эдвардом Лопером в 2001 году и вскоре стала очень популярной среди исследователей и академиков. Библиотека предоставляет инструменты для лингвистического анализа, также содержит предварительно обученные модели и множество корпусов. NLTK имеет подробную документацию по API и доступные руководства по её использованию. К главным недостаткам библиотеки можно отнести сложность в освоении и использовании, медлительность, отсутствие нейросетевых моделей.

Самые распространённые реализации алгоритма градиентного бустинга содержатся в следующих библиотеках: CatBoost, LightGBM, XGBoost, Scikit-learn. XGBoost была разработана как исследовательский проект Чэн Тяньци в 2014 году, LightGBM компанией Microsoft в 2016 году, CatBoost компанией Yandex в 2017 году. Основное различие между ними заключается в реализации данного алгоритма.

# **Применение моделей представления текстовых документов и классификаторов к выбранным корпусам текстовых документов** <a name="res"></a>

На основании данных качества решения задачи классификации из научных статей были выбраны следующие нейросетевые модели: C-LSTM, RMDL, BERT_BASE.

В качестве алгоритмов машинного обучения были выбраны следующие: случайный лес, метод опорных векторов и градиентный бустинг. Во всех этих алгоритмах использовались стандартные гиперпараметры.

Для реализации C-LSTM и BERTBASE использовались PyTorch и transformers, а для RMDL библиотека от авторов этой модели. В качестве модели BERT_BASE использовалась оригинальная предобученная модель без учёта регистра букв.

Из-за возникших проблем с обучением модели C-LSTM она была модифицирована: слой Dropout применяется только к эмбидингам, Softmax на выходе модели отсутствует, в слое свёртки используется stride равным 3, в функции потерь используются весовые коэффициенты обратно пропорциональные частоте классов, максимальная длина документов для паддинга вычисляется на основе батча.

Из-за устаревшего кода библиотеки RMDL в неё были внесены несущественные изменения исправляющие возникшие программные ошибки и повышающие производительность вычислений.

Для C-LSTM и алгоритмов машинного обучения предобработка текста заключалась в удалении специальных символов в текстовом документе.

В качестве модели представления текстовых документов для C-LSTM использовался Word2Vec предобученный на Google News, а для RMDL — GloVe предобученный на Википедии 2014 года и наборе Gigaword 5 с размерностью эмбидингов равной 50.

В качестве алгоритма градиентного бустинга использовались реализации библиотек из LightGBM и CatBoost, а для метода опорных векторов и случайного леса использовались реализации из scikit-learn.

Чаще всего для этих алгоритмов в качестве модели представления текстовых документов используется TF-IDF, однако при большом количестве текстовых документов и словаре возникает проблема чрезмерного потребления оперативной памяти, поэтому в данной работе текстовый документ представлялся как сумма векторов слов этого документа из модели Word2Vec предобученной на Google News, если же слово отсутствовало в модели, то использовался нулевой вектор.

Для упрощения исследований на корпусе WOS в качестве классов использовались метки первого уровня. Разбиение корпуса WOS производилось на обучающую и тестовую выборки в соотношении 67/33 %, а корпуса Ohsumed на обучающую, валидационную и тестовую в соотношении 80/10/10 %; при этом при обучении алгоритмов машинного обучения на корпусе Ohsumed валидационная выборка была совмещена с тестовой.

<table>
    <thead>
        <tr>
            <td rowspan=2 align="center">Модель</td>
            <td colspan=4 align="center">Размер словаря / Количество параметров модели</td>
        </tr>
        <tr>
            <td align="center">Ohsumed</td>
            <td align="center">WOS</td>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td align="center">С-LSTM</td>
            <td align="center">85,325 / 25,917,32</td>
            <td align="center">157,995 / 47,735,084</td>
        </tr>
        <tr>
            <td align="center">RMDL</td>
            <td align="center">11,000 – 65,862 / 19,907,572</td>
            <td align="center">12,500 – 204,026 / 34,882,188</td>
        </tr>
    </tbody>
</table>

<table>
    <thead>
        <tr>
            <td rowspan=2 align="center">Модель</td>
            <td colspan=4 align="center">Accuracy / Macro-F1-Score / Время обучения</td>
        </tr>
        <tr>
            <td align="center">Ohsumed</td>
            <td align="center">WOS</td>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td align="center">Метод опорных векторов</td>
            <td align="center">0.39 / 0.29 / 9 мин 49 с</td>
            <td align="center">0.77 / 0.75 / 2 мин 9 с</td>
        </tr>
        <tr>
            <td align="center">Случайный лес</td>
            <td align="center">0.16 / 0.08 / 19 с</td>
            <td align="center">0.71 / 0.67 / 10.6 с</td>
        </tr>
        <tr>
            <td align="center">LightGBM</td>
            <td align="center">0.23 / 0.16 / 1 мин 2 с</td>
            <td align="center">0.75 / 0.73 / 17.2 с</td>
        </tr>
        <tr>
            <td align="center">CatBoost</td>
            <td align="center">0.25 / 0.19 / 52.3 с</td>
            <td align="center">0.76 / 0.75 / 26 с</td>
        </tr>
        <tr>
            <td align="center">C-LSTM</td>
            <td align="center">0.33 / 0.24 / 39 мин</td>
            <td align="center">0.76 / 0.74 / 12 мин 38 c</td>
        </tr>
        <tr>
            <td align="center">BERT_BASE</td>
            <td align="center">0.45 / 0.41 / 2 ч 30 мин</td>
            <td align="center">0.89 / 0.89 / 3 ч 45 мин</td>
        </tr>
        <tr>
            <td align="center">RMDL</td>
            <td align="center">0.44 / 0.39 / 1 ч 30 мин</td>
            <td align="center">0.88 / 0.88 / 1 ч 30 мин</td>
        </tr>
    </tbody>
</table>


# **Заключение** <a name="conc"></a>

В рамках научно-исследовательской работы была представлена актуальность задачи классификации текстовых документов, описана её постановка и рассмотрены различные методы обработки текстовых документов в контексте рассматриваемой задачи.

Произведён выбор корпусов текстовых документов Ohsumed и WOS для решения поставленной задачи.

Перечислены популярные программные решения для решения задач обработки естественного языка.

Проведено практическое исследование качества классификации различных классификаторов текстовых документов. В качестве классификаторов для выбранных корпусов текстов Ohsumed и WOS использовались: метод опорных векторов, случайный лес, градиентный бустинг, C-LSTM, RMDL и BERT.

Лучший результат для обоих корпусов с точки зрения качества решения задачи показала модель BERT, однако метод опорных векторов дал сравнительно близкий к ней результат при намного меньших временных затратах на обучение.

Дальнейшие исследования включают в себя:
- анализ эффективности обученных моделей в реальном использовании;
- подбор гиперпараметров методов машинного обучения;
- классификация корпуса WOS по двум уровням;
- синтез наилучшего классификатора;
- ускорение моделей и уменьшение их размера;
- многозначная (нечёткая) многоклассовая классификация корпуса arXiv на 2.2 млн аннотаций научных статей;


# **Список литературы** <a name="lit"></a>

1.	Shervin Minaee, Nal Kalchbrenner, Erik Cambria, Narjes Nikzad, Meysam Chenaghlu, and Jianfeng Gao. 2020. Deep Learning Based Text Classification: A Comprehensive Review.
2.	Батура, Т.В. МЕТОДЫ АВТОМАТИЧЕСКОЙ КЛАССИФИКАЦИИ ТЕКСТОВ.
3.	Vapnik, V.; Chervonenkis, A.Y. A class of algorithms for pattern recognition learning. Avtomat. Telemekh 1964.
4.	Boser, B.E.; Guyon, I.M.; Vapnik, V.N. A training algorithm for optimal margin classifiers. In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, Pittsburgh, PA, USA, 27–29 July 1992.
5.	J. Weston, C. Watkins, Multi-class support vector machines (Technical report, Citeseer, 1998).
6.	Cortes, Corinna, and Vladimir Vapnik. "Support-vector networks." Machine learning 20.3 (1995).
7.	Chih-Chung Chang and Chih-Jen Lin, LIBSVM: a library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27, 2011.
8.	Morgan, J.N.; Sonquist, J.A. Problems in the analysis of survey data, and a proposal. J. Am. Stat. Assoc. 1963.
9.	T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning: data mining, inference, and prediction. 2nd Edition. Springer Science & Business Media, 2009.
10.	Ho, T.K. Random decision forests. In Proceedings of the 3rd International Conference on Document Analysis and Recognition, Montreal, QC, Canada 14–16 August 1995; Volume 1, pp. 278–282.
11.	Breiman, L. (1996) Bagging Predictors. Machine Learning, 24, 123-140.
12.	Breiman, L. Random Forests; UC Berkeley TR567; University of California: Berkeley, CA, USA, 1999.
13.	Kowsari, Kamran & Jafari Meimandi, Kiana & Heidarysafa, Mojtaba & Mendu, Sanjana & Barnes, Laura & Brown, Donald. (2019). Text Classification Algorithms: A Survey.
14.	M. Kearns, “Thoughts on Hypothesis Boosting”, Unpublished, Machine Learning Class Project, 1988.
15.	Schapire, R.E. The strength of weak learnability. Mach Learn 5, 197–227 (1990).
16.	Friedman, Jerome H.. “1999 REITZ LECTURE GREEDY FUNCTION APPROXIMATION: A GRADIENT BOOSTING MACHINE'.” (2001).
17.	Friedman, J.H. (2002). Stochastic gradient boosting. Computational Statistics & Data Analysis, 38, 367-378.
18.	K. Kowsari, D. E. Brown, M. Heidarysafa, K. Jafari Meimandi, M. S. Gerber, and L. E. Barnes, “HDLTex: Hierarchical deep learning for text classification,” 2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA), Dec 2017.
19.	https://colah.github.io/posts/2015-08-Understanding-LSTMs
20.	Hochreiter, S.; Schmidhuber, J. Long short-term memory. Neural Comput. 1997, 9, 1735–1780.
21.	Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F.; Schwenk, H.; Bengio, Y. Learning phrase representations using RNN encoder-decoder for statistical machine translation.
22.	P. Zhou, Z. Qi, S. Zheng, J. Xu, H. Bao, and B. Xu, “Text classification improved by integrating bidirectional lstm with two-dimensional max pooling”, 2016.
23.	Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning applied to document recognition,” Proceedings of the IEEE, vol. 86, no. 11, pp. 2278–2324, 1998.
24.	Collobert, R.; Weston, J. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th International Conference on Machine Learning, Helsinki, Finland, 5–9 July 2008; pp. 160–167.
25.	Kalchbrenner, N.; Grefenstette, E.; Blunsom, P. A convolutional neural network for modelling sentences, 2014.
26.	Y. Kim, “Convolutional neural networks for sentence classification” in EMNLP 2014 - 2014 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference, 2014.
27.	Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-Level Convolutional Networks for Text Classification. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1 (Montreal, Canada) (NIPS’15). MIT Press, Cambridge, MA, USA, 649–657.
28.	Baldi, P. Autoencoders, unsupervised learning, and deep architectures. In Proceedings of the ICMLWorkshop on Unsupervised and Transfer Learning, Bellevue, WA, USA, 2 July 2011; pp. 37–49.
29.	Bahdanau, D.; Cho, K.; Bengio, Y. Neural machine translation by jointly learning to align and translate. 2014.
30.	Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A.N.; Kaiser, Ł.; Polosukhin, I. Attention Is All You Need. 2017.
31.	Radford, A.; Narasimhan, K.; Salimans, T.; Sutskever, I. Improving Language Understanding by GenerativePre-Training. 2018.
32.	Devlin, J.; Chang, M.W.; Lee, K.; Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. 2018.
33.	Lai, S.; Xu, L.; Liu, K.; Zhao, J. Recurrent Convolutional Neural Networks for Text Classification. In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, Austin, TX, USA, 25–30 January 2015; Volume 333, pp. 2267–2273.
34.	Zhou, C.; Sun, C.; Liu, Z.; Lau, F. A C-LSTM neural network for text classification. 2015.
35.	G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov, “Improving neural networks by preventing co-adaptation of feature detectors”, Jul. 2012.
36.	D. Tang, B. Qin, and T. Liu, “Document modeling with gated recurrent neural network for sentiment classification,” in Proceedings of the 2015 conference on empirical methods in natural language processing, 2015, pp. 1422–1432.
37.	G. Chen, D. Ye, E. Cambria, J. Chen, and Z. Xing, “Ensemble application of convolutional and recurrent neural networks for multi-label text categorization,” in IJCNN, 2017, pp. 2377–2383.
38.	Kowsari, K.; Heidarysafa, M.; Brown, D.E.; Jafari Meimandi, K.; Barnes, L.E. RMDL: Random Multimodel Deep Learning for Classification. In Proceedings of the 2018 International Conference on Information System and Data Mining, Lakeland, FL, USA, 9–11 April 2018.
39.	Yang, Z.; Yang, D.; Dyer, C.; He, X.; Smola, A.J.; Hovy, E.H. Hierarchical Attention Networks for Document Classification. In Proceedings of the HLT-NAACL, San Diego, CA, USA, 12–17 June 2016; pp. 1480–1489.
40.	B. Yang and Z. Xiao,“A multi-channel and multi-spatial attention convolutional neural network for prostate cancer ISUP grading,” Applied Sciences, vol. 11, no. 10, pp. 4321–4330, 2021.
41.	K. Babić, S. Martinčić-Ipšić, and A. Meštrović, Survey of neural text representation models, Information 11, 511 (2020).
42.	S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, and R. Harshman, “Indexing by latent semantic analysis,” Journal of the American society for information science, vol. 41, no. 6, pp. 391–407, 1990.
43.	Mikolov, T.; Chen, K.; Corrado, G.; Dean, J. Efficient estimation of word representations in vector space. 2013.
44.	Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G.S.; Dean, J. Distributed representations of words and phrases and their compositionality. Adv. Neural Inf. Process. Syst. 2013, 26, 3111–3119.
45.	Rong X. word2vec parameter learning explained. 2014.
46.	Pennington, J.; Socher, R.; Manning, C.D. Glove: Global Vectors forWord Representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Doha, Qatar, 25–29 October 2014; Volume 14, pp. 1532–1543.
47.	A. Joulin, E. Grave, P. Bojanowski, and T. Mikolov, “Bag of tricks for efficient text classification”, 2016.
48.	Bojanowski, P.; Grave, E.; Joulin, A.; Mikolov, T. Enriching word vectors with subword information. 2016.
49.	Melamud, O.; Goldberger, J.; Dagan, I. context2vec: Learning generic context embedding with bidirectional lstm. In Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, Berlin, Germany, 11–12 August 2016; pp. 51–61.
50.	Peters, M.E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; Zettlemoyer, L. Deep contextualized word representations. 2018.
51.	Beltagy, I.; Lo, K.; Cohan, A. SciBERT: A Pretrained Language Model for Scientific Text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Hong Kong, China, 3–7 November 2019; pp. 3615–3620.
52.	Lee J, et al. BioBERT: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics. 2020;36(4):1234–40.
53.	Shen, S., Dong, Z., Ye, J., Ma, L., Yao, Z., Gholami, A., Mahoney, M.W., Keutzer, K. Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT. 2019.
54.	Sanh, Victor & Debut, Lysandre & Chaumond, Julien & Wolf, Thomas. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. 2019.
55.	Hersh, W.; Buckley, C.; Leone, T.J.; Hickam, D. OHSUMED: An Interactive Retrieval Evaluation and New Large Test Collection for Research. In SIGIR ’94; Springer: London, UK, 1994; pp. 192–201.
56.	T. Joachims. Text Categorization with Support Vector Machines // Proc. European Conf. Machine Learning (ECML'98), 1998.
57.	Pang, B.; Lee, L. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, Ann Arbor, MI, USA, 17 June 2005; pp. 115–124.
58.	Socher, R.; Perelygin, A.; Wu, J.; Chuang, J.; Manning, C.D.; Ng, A.; Potts, C. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Seattle,WA, USA, 18–21 October 2013; pp. 1631–1642.
59.	Pang, B.; Lee, L. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd annual meeting on Association for Computational Linguistics. Association for Computational Linguistics, Barcelona, Spain, 21–26 July 2004; p. 271.
60.	Voorhees, E.M.; Harman, D. Overview of TREC 2002. In Proceedings of the Eleventh Text REtrieval Conference, Gaithersburg, MD, USA, 19–22 November 2002.
61.	Xin Li and Dan Roth. 2002. Learning Question Classifiers. In COLING 2002: The 19th International Conference on Computational Linguistics.
62.	Maas, A.L.; Daly, R.E.; Pham, P.T.; Huang, D.; Ng, A.Y.; Potts, C. Learning Word Vectors for Sentiment Analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Portland, OR, USA, 19–24 June 2011; Association for Computational Linguistics: Portland, OR, USA, 2011; pp. 142–150.
63.	K. Lang, “NewsWeeder: learning to filter netnews,” in Proceedings of the 12th International Conference on Machine Learning. Morgan Kaufmann publishers Inc.: San Mateo, CA, USA, 1995, pp. 331–339.
64.	Qiming Diao, Minghui Qiu, Chao-Yuan Wu, Alexander J Smola, Jing Jiang, and Chong Wang. 2014. Jointly modeling aspects, ratings and sentiments for movie recommendation (jmars). In SIGKDD, pages193–202. ACM.
65.	D. Lewis, “Reuters-21578 text categorization test collection,” 1997.
66.	Lewis, D. D.; Yang, Y.; Rose, T.; and Li, F. RCV1: A New Benchmark Collection for Text Categorization Research. Journal of Machine Learning Research, 5:361-397, 2004.
67.	D. D. Lewis, Y. Yang, T. G. Rose, and F. Li, “Rcv1: A new benchmark collection for text categorization research,” Journal of machine learning research, vol. 5, no. Apr, pp. 361–397, 2004.
68.	Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.
69.	Wagner, W. Steven Bird, Ewan Klein and Edward Loper: Natural Language Processing with Python, Analyzing Text with the Natural Language Toolkit. Lang Resources & Evaluation, 44, 421–424, 2010.
70.	Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Rafal Jozefowicz, Yangqing Jia, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mané, Mike Schuster, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems. 2015.
71.	Paszke, Adam, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai and Soumith Chintala. “PyTorch: An Imperative Style, High-Performance Deep Learning Library”, 2019.
72.	F. Chollet, et al. Keras [Internet]. GitHub; 2015.
73.	Wolf, Thomas & Debut, Lysandre & Sanh, Victor & Chaumond, Julien & Delangue, Clement & Moi, Anthony & Cistac, Pierric & Rault, Tim & Louf, Remi & Funtowicz, Morgan & Davison, Joe & Shleifer, Sam & Platen, Patrick & Ma, Clara & Jernite, Yacine & Plu, Julien & Xu, Canwen & Scao, Teven & Gugger, Sylvain & Rush, Alexander. Transformers: State-of-the-Art Natural Language Processing, 38-45, 2020.
74.	Lhoest, Quentin & Moral, Albert & Jernite, Yacine & Thakur, Abhishek & Platen, Patrick & Patil, Suraj & Chaumond, Julien & Drame, Mariama & Plu, Julien & Tunstall, Lewis & Davison, Joe & Šaško, Mario & Chhablani, Gunjan & Malik, Bhavitvya & Brandeis, Simon & Scao, Teven & Sanh, Victor & Xu, Canwen & Patry, Nicolas & Wolf, Thomas. Datasets: A Community Library for Natural Language Processing. 2021.
75.	Honnibal, M., & Montani, I. spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing. 2017.
76.	Radim Rehurek and Petr Sojka. Software framework for topic modelling with large corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, 2010.
77.	Harris, C.R., Millman, K.J., van der Walt, S.J. et al. Array programming with NumPy. Nature 585, 357–362, 2020.
78.	Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stéfan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, CJ Carey, İlhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E.A. Quintero, Charles R Harris, Anne M. Archibald, Antônio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17(3), 261-272, 2020.
79.	Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton and Christopher D. Manning. Stanza: A Python Natural Language Processing Toolkit for Many Human Languages. In Association for Computational Linguistics (ACL) System Demonstrations. 2020.
80.	Manning, Christopher D., Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. The Stanford CoreNLP Natural Language Processing Toolkit. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 55-60, 2014.
81.	Gardner, Matt, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson H S Liu, Matthew E. Peters, Michael Schmitz and Luke Zettlemoyer. A Deep Semantic Natural Language Processing Platform. 2017.
82.	Alan Akbik, Tanja Bergmann, Duncan Blythe, Kashif Rasul, Stefan Schweter, and Roland Vollgraf. FLAIR: An Easy-to-Use Framework for State-of-the-Art NLP. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 54–59, Minneapolis, Minnesota. Association for Computational Linguistics. 2019.
83.	Burtsev, Mikhail S., Alexander V. Seliverstov, Rafael Airapetyan, Mikhail Arkhipov, Dilyara Baymurzina, Nickolay Bushkov, Olga Gureenkova, Taras Khakhulin, Yuri Kuratov, Denis Kuznetsov, Alexey Litinsky, Varvara Logacheva, Alexey Lymar, Valentin Malykh, Maxim Petrov, Vadim Polulyakh, Leonid Pugachev, A. Sorokin, Maria Vikhreva and Marat Zaynutdinov. DeepPavlov: Open-Source Library for Dialogue Systems. Annual Meeting of the Association for Computational Linguistics. 2018.
84.	V. Kocaman and D. Talby. Spark NLP: natural language understanding at scale. Softw. Impacts, vol. 8, p. 100058, 2021.
85.	https://habr.com/ru/post/516098/
86.	Korobov M.: Morphological Analyzer and Generator for Russian and Ukrainian Languages // Analysis of Images, Social Networks and Texts, pp 320-332, 2015.
87.	Segalovich, I. A. Fast Morphological Algorithm with Unknown Word Guessing Induced by a Dictionary for a Web Search Engine. MLMTA. 2003.
88.	Dorogush A.V., Ershov V., Gulin A. CatBoost: gradient boosting with categorical features support. 2018.
89.	Ostroumova L., Gusev G., Vorobev A., Dorogush A.V., Gulin A. CatBoost: unbiased boosting with categorical features. Neural Information Processing Systems. 2018.
90.	Ke G., Meng Q., Finley T., Wang T., Chen W., Ma W., Ye Q., Liu T. LightGBM: A Highly Efficient Gradient Boosting Decision Tree. Advances in Neural Information Processing Systems 30 (NIPS 2017), pp. 3149-3157, 2017.
91.	Tianqi, C.; Carlos, G. XGBoost: A Scalable Tree Boosting System. In Proceedings of the Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ACM, San Francisco, CA, USA, 13–17 August 2016; pp. 785–794.
92.	Rahman T. vocabulary Documentation. Release 10. 2017.
93.	De Smedt, T.; Daelemans, W. Pattern for Python. Journal of Machine Learning Research. 2012, 13, 2063–2067.
94.	R. Al-Rfou, B. Perozzi, S. Skiena. Polyglot: Distributed Word Representations for Multilingual NLP. 2014.
95.	Gompel M.V., Saphra N., Fischer S., Atteveldt W.V., Sloot K.V., Martijn, Beka A., Scheidler B. pynlpl: v1.2.9. 2019.
96.	Loria S. textblob Documentation. Release 0.17.1, 2. 2021.
97.	https://github.com/Novetta/adaptnlp
98.	Ling, M. H. T. An Anthological Review of Research Utilizing MontyLingua, a Python-Based End-to-End Text Processor. The Python Papers, 1, 5-12, 2006.
99.	Joao Silva, Luisa Coheur, Ana Cristina Mendes, and Andreas Wichert. 2011. From symbolic to sub-symbolic information in question classification. Artificial Intelligence Review, 35(2):137–154.
100.	Kai Sheng Tai, Richard Socher, and Christopher D Manning. 2015. Improved semantic representations from tree-structured long short-term memory networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics, pages 1556–1566.
101.	W. Zhao, J. Ye, M. Yang, Z. Lei, S. Zhang, and Z. Zhao, “Investigating Capsule Networks with Dynamic Routing for Text Classification” 2018.
102.	Y. Yang, “Convolutional Neural Networks with Recurrent Neural Filters” Proc. of 2018 Conference on Empirical Methods in Natural Language Processing, 2018.
103.	Munikar, M.; Shakya, S.; Shrestha. A Fine-grained Sentiment Classification using BERT. In Proceedings of the 2019 Artificial Intelligence for Transforming Business and Society (AITB), Kathmandu, Nepal, 5 November 2019; pp. 1–5, Volume 1.
104.	C. Sun, X. Qiu, Y. Xu, and X. Huang, “How to fine-tune bert for text classification?” in China National Conference on Chinese Computational Linguistics. Springer, 2019, pp. 194-206.
105.	V. Sanh, L. Debut, J. Chaumond, T. Wolf. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. 2019.

---

Данные находятся по [ссылке](https://drive.google.com/drive/folders/154wkjnCGjnFVQ_chMMJcOrCYMAnzwSgM?usp=sharing).
Датасет Ohsume можно скачать [здесь](http://disi.unitn.it/moschitti/corpora.htm) (All Cardiovascular diseases abstracts).

Работа находится в процессе выполнения и постепенно результаты будут добавляться сюда.

---

(c) Ларчев В.И., 2022.

Научный руководитель - Толчеев В.О.
